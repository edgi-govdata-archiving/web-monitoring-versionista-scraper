#!/usr/bin/env node
'use strict';

const {compareMany, ascend, descend, getDeep} = require('../lib/tools');
const flatten = require('../lib/flatten');
const formatCsv = require('../lib/formatters/csv');
const fs = require('fs-promise');
const neodoc = require('neodoc');
const nodemailer = require('nodemailer');
const path = require('path');
const qs = require('qs');
const request = require('request');
const spawn = require('child_process').spawn;
const stream = require('stream');

const args = neodoc.run(`
Query a web-monitoring-db instance for pages that were updated with new
versions during a given time frame and e-mail a compressed \`.tar.gz\` archive
of the results to a specified address.
Results are CSV files -- one per combination of tags specified with --group-by.

Usage: query-db-and-email [--group-by TAG]... [options]

Options:
  -h, --help              Print this lovely help message.
  --after HOURS           Only include versions from N hours ago. [default: 72]
  --before HOURS          Only include versions from before N hours ago.
  --db-url STRING         URL for web-monitoring-db instance [env: WEB_MONITORING_URL]
                          [default: https://api.monitoring.envirodatagov.org/]
  --ui-url STRING         URL for web-monitoring-ui instance [env: WEB_MONITORING_UI_URL]
                          [default: https://monitoring.envirodatagov.org/]
  --link-to-versionista   Create view/diff links to Versionista instead of Web-Monitoring
  --source-type TYPE      Restrict query to only versions with the given
                          \`source_type\` field (as in the DB API). Use \`ANY\`
                          for any source type. [default: versionista]
  --output DIRECTORY      Write output to this directory.
  --sender-email STRING   E-mail address to send from. [env: SEND_ARCHIVES_FROM]
  --sender-password PASS  Password for e-mail account to send from. [env: SEND_ARCHIVES_PASSWORD]
  --receiver-email STRING E-mail address to send results to [env: SEND_ARCHIVES_TO]
  --chunk-size NUMBER     Number of records to fetch at a time [default: 100]
  --chunk-delay SECONDS   Number of seconds to wait between chunks [default: 0]
  --debug                 Print debug messages
  --group-by TAG          Group resulting sheets by tag prefix. [default: site:]
`);

process.on('unhandledRejection', (reason, p) => {
  console.error('Unhandled Rejection at:', p, 'reason:', reason);
  console.error(reason.stack);
});

const scriptsPath = __dirname;
const outputParent = path.resolve(args['--output']);
const dbUrl = (args['--db-url'])
  .split(',')
  .map(dbUrl => dbUrl.trim())
  .filter(dbUrl => !!dbUrl)
  .map(dbUrl => dbUrl.endsWith('/') ? dbUrl : (dbUrl + '/'))
  [0];
const dbCredentials = getCredentialsFromUrl(dbUrl);
const uiUrl = args['--ui-url'] + (args['--ui-url'].endsWith('/') ? '' : '/');
const linkToVersionista = args['--link-to-versionista'];
const tagGroups = args['--group-by'];

let sourceType = args['--source-type'];
if (sourceType === 'ANY') {
  sourceType = undefined;
}

const senderEmailName = 'EDGI Versionista Scraper'
const scrapeTime = new Date();
// Use ISO Zulu time without seconds in our time strings
const timeString = scrapeTime.toISOString().slice(0, 16) + 'Z';
const safeScrapeTime = timeString.replace(/:/g, '-');
const safeTagGroups = tagGroups.map(group => group.replace(/:/g, '')).join('-');
const outputDirectory = path.join(outputParent, `webmonitoring-${safeTagGroups}s-${safeScrapeTime}`);
const requestCache = path.join(outputParent, '.cache');


let startTime;
if (typeof args['--after'] === 'string' && args['--after'].includes('-')) {
  startTime = new Date(args['--after']);
  if (isNaN(startTime)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
}
else {
  const hours = Number(args['--after'])
  if (isNaN(hours)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
  startTime = new Date(Date.now() - hours * 60 * 60 * 1000);
}

let endTime = scrapeTime;
if (args['--before']) {
  if (typeof args['--before'] === 'string' && args['--before'].includes('-')) {
    endTime = new Date(args['--before']);
    if (isNaN(endTime)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
  }
  else {
    const hours = Number(args['--before'])
    if (isNaN(hours)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
    endTime = new Date(Date.now() - hours * 60 * 60 * 1000);
  }
}

const chunkDelay = Math.max(0, Number(args['--chunk-delay'])) * 1000;
const chunkSize = Number(args['--chunk-size']) || 100;

class SetMap extends Map {
  get (key) {
    let value = super.get(key);
    if (!value) {
      value = new Set();
      this.set(key, value);
    }
    return value;
  }

  add (key, item) {
    return this.get(key).add(item);
  }
}

let removeOutput = true;
fs.ensureDir(outputDirectory)
  .then(() => getGroupUpdates())
  .then(result => writeCsvsForGroups(result.pagesByGroup).then(() => result))
  .then(result => {
    return compressPath(outputDirectory).then(compressed => ({
      text: `Found ${result.groupCount} groups with updates
Found ${result.pageCount} pages with updates
Completed in ${result.queryDuration / 1000} seconds`,
      path: compressed
    }));
  })
  .catch(error => {
    // keep output for debugging if there was an error
    removeOutput = false;
    // write to console, but also send in e-mail
    console.error(error.stack || error)
    return error;
  })
  .then(async results => {
    await clearCache(requestCache);
    return results;
  })
  .then(sendResults)
  .catch(error => console.error(error.stack || error))
  .then(() => {
    if (removeOutput) {
      run(`rm`, ['-rf', path.join(outputParent, '*')], {shell: true});
    }
  });

function createViewUrl (page, toVersion, fromVersion, useVersionista) {
  if (useVersionista != null ? useVersionista : linkToVersionista) {
    const metadata = (toVersion || page.latest).source_metadata;
    let url = `https://versionista.com/${metadata.site_id}/${metadata.page_id}/`;
    if (fromVersion) {
      url = metadata.diff_with_first_url;
    }
    else if (toVersion) {
      url = metadata.diff_with_previous_url;
    }
    if (url && !url.endsWith('/')) {
      url = url + '/';
    }
    return url || '';
  }

  let url = `${uiUrl}page/${page.uuid}/`;
  if (toVersion) {
    url += `${fromVersion ? fromVersion.uuid : ''}..${toVersion.uuid}`;
  }

  return url;
}

function getGroupUpdates () {
  const pagesByGroup = new SetMap();

  const timeframePages = getAllResults('api/v0/pages', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize,
    include_earliest: true,
    active: true
  })
  .catch(error => {
    console.error(`Failed to get all pages: ${error}`);
    return Promise.reject(error);
  });

  // Looking up all the versions in one shot is much more efficient than
  // looking up the versions for each page individually
  const timeframeVersions = getAllResults('api/v0/versions', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize,
    sort: 'capture_time:desc',
    different: true,
    include_change_from_previous: true
  })
  .catch(error => {
    console.error(`Failed to get all versions: ${error}`);
    return Promise.reject(error);
  });

  return Promise.all([timeframePages, timeframeVersions])
    .then(([pages, versions]) => {
      // Create a lookup table for pages
      const pagesById = new Map();
      pages.forEach(page => {
        pagesById.set(page.uuid, page)
        page.versions = [];
      });

      // Attach each version to its page
      versions.forEach(version => {
        const page = pagesById.get(version.page_uuid);
        if (!page) {
          console.error(`Warning: no matching page found for version ${version.uuid}`);
          return;
        }
        page.versions.push(version);
      });

      return pages;
    })
    // Group pages based on tags
    .then(pages => {
      pages.forEach(page => {
        // Condense groups into a single string (each group separated by `--`)
        page.group = page.tags
          .reduce((groups, tag) => {
            tagGroups.some((group, index) => {
              if (tag.name.startsWith(group)) {
                const groupName = (group === tag.name) ? tag.name : tag.name.slice(group.length);
                groups[index] = groupName;
                return true;
              }
            });
            return groups;
          }, [])
          .map(group => (group || ''))
          .join('--');

        const latest = page.versions[0];
        page.latest = latest;
        if (!latest) {
          // Don't add a page to the list if it has no versions.
          console.warn(`Found page with no versions: ${page.uuid}`);
          return;
        }

        // Check whether there was also a non-error version that we should show
        if (isError(latest)) {
          pagesByGroup.add('errors', page);

          for (let i = 1, len = page.versions.length; i < len; i++) {
            const version = page.versions[i];
            if (!isError(version)) {
              const nonErrorPage = Object.assign({}, page, {latest: version});
              pagesByGroup.add(page.group, nonErrorPage);
              break;
            }
          }
        }
        else {
          pagesByGroup.add(page.group, page);
        }
      });

      return {
        pagesByGroup,
        pageCount: pages.length,
        groupCount: pagesByGroup.size,
        queryDuration: Date.now() - scrapeTime.getTime()
      };
    });
}

function isError (version) {
  return version.source_metadata.errorCode
    || version.source_metadata.error_code;
}

function writeCsvsForGroups (pagesByGroup) {
  return Promise.all([...pagesByGroup].map(([group, pages]) => {
    const csv = csvStringForPages([...pages]);
    const filename = `${group}_${safeScrapeTime}.csv`.replace(/[:/]/g, '_');
    return fs.writeFile(path.join(outputDirectory, filename), csv);
  }));
}

// Create an object representing an annotation, whether one was present or not
function compileAnnotation (version) {
  const meta = version.source_metadata || {};
  return Object.assign({
    source_diff_length: meta.diff_length || 0,
    source_diff_hash: meta.diff_hash || '?',
    text_diff_length: meta.text_diff_length || 0,
    text_diff_hash: meta.text_diff_hash || '?'
  }, getDeep(version, 'change_from_previous', 'current_annotation'));
}

// Merge a set of annotation objects to summarize the analysis for all changes
// that occurred over the time period.
function mergeAnnotations (versions, annotations) {
  // If the start and end are OK pages, we want to skip counting the stats from
  // any intermittent errors in between. Sometimes we'll record an intermittent
  // server error or a bad deploy on a website, so it'll look like a massive
  // change. If the error goes away again, though, it shouldn't count, since
  // the end-to-end comparison will be much less of a big deal.
  const skipErrors = versions.length >= 1
    && getStatusCode(versions[0]) < 300
    && getStatusCode(versions[versions.length - 1]) < 300;
  let previousWasError = false;
  // Go in ascending date order because we need to track whether the previous
  // was an error.
  versions = versions.slice().reverse();
  annotations = annotations.slice().reverse();

  // NOTE: annotations will come is descending order of time. Should we sort?
  const base = Object.assign({}, annotations[0]);
  return annotations.slice(1).reduce((merged, annotation, index) => {
    // If we are skipping errors and this or the previous was an error, skip.
    const isError = getStatusCode(versions[index + 1]) >= 300;
    if (skipErrors && (isError || previousWasError)) {
      previousWasError = isError;
      return merged;
    }

    // Just take the latest hash; not much useful we can do to combine them.
    if (!isChangeHash(merged.source_diff_hash)) {
      merged.source_diff_hash = annotation.source_diff_hash;
    }
    if (!isChangeHash(merged.text_diff_hash)) {
      merged.text_diff_hash = annotation.text_diff_hash;
    }

    // Take the highest priority, but don't set priority if it's unknown.
    if (annotation.priority != null) {
      merged.priority = Math.max(merged.priority || 0, annotation.priority);
    }

    // Sum the lengths.
    merged.source_diff_length += annotation.source_diff_length;
    merged.text_diff_length += annotation.text_diff_length;

    return merged;
  }, base);
}

function getStatusCode (version) {
  // This only supports Wayback-sourced data, but that's OK because it's all we
  // we have right now.
  return version && version.source_metadata.status_code || 200;
}

function csvStringForPages (pages) {
  const blankVersion = {uuid: '', source_metadata: {}};
  const entries = pages
    .map(page => {
      const earliest = page.earliest || blankVersion;
      const version = page.latest || blankVersion;
      earliest.capture_time = parseDate(earliest.capture_time);
      version.capture_time = parseDate(version.capture_time);
      const annotations = page.versions.map(compileAnnotation);
      const annotation = mergeAnnotations(page.versions, annotations);
      return {page, earliest, version, annotation};
    });

  // Creates a CSV row from an entry object (above)
  const createRow = ({page, earliest, version, annotation}, index) => {
    // If we have change objects, we can use them to find the UUID of the
    // latest version before the timeframe we queried. This'll be used to
    // compose diff links that cover the whole timeframe.
    const beforeTimeframe = getDeep(
      page.versions[page.versions.length - 1],
      'change_from_previous',
      'uuid_from') || '';

    return [
      index + 1,
      version.uuid,
      // TODO: format
      timeString,
      page.maintainers.map(maintainer => maintainer.name).join(', '),
      page.group,
      cleanString(page.title),
      page.url,
      createViewUrl(page),
      createViewUrl(page, version, {uuid: beforeTimeframe}),
      createViewUrl(page, version, earliest),
      formatCsv.formatDate(version.capture_time),
      formatCsv.formatDate(earliest.capture_time) || '----',
      annotation.source_diff_length,
      formatHash(annotation.source_diff_hash),
      annotation.text_diff_length,
      formatHash(annotation.text_diff_hash),
      page.versions.length,
      annotation.priority
    ];
  };

  // Group rows by text hash and sort those groups by their maximum priority.
  // Basically we want something like a priority sort, but we want to make sure
  // identical hashes stay together.
  const textHashGroups = new Map();
  entries.forEach(entry => {
    let group = textHashGroups.get(entry.annotation.text_diff_hash);
    if (!group) {
      group = [];
      textHashGroups.set(entry.annotation.text_diff_hash, group);
    }
    group.push(entry);
    group.priority = Math.max(group.priority || 0, entry.annotation.priority || 0);
  });

  const sortedGroups = Array.from(textHashGroups.values())
    .sort(descend('priority'))
    .map(group => group.sort(compareMany(
      ascend(x => x.annotation.source_diff_hash),
      descend(x => x.annotation.priority),
      ascend(x => x.version.capture_time)
    )));
  const sortedRows = flatten(sortedGroups).map(createRow);

  const headers = [...formatCsv.headers, 'number of versions', 'priority'];
  // "Last two" is customized to cover the whole period in this script.
  headers[8] = 'This Period - Side by Side';
  return formatCsv.toCsvString([headers, ...sortedRows]);
}

function parseDate (date) {
  return date && new Date(date);
}

// Hash of an empty string
const hashEmptyString = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855';
// Hash of an empty JSON array (`[]`)
const hashEmptyArray = '4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945';

function isChangeHash (candidate) {
  return candidate
    && candidate !== '?'
    && candidate !== hashEmptyString
    && candidate !== hashEmptyArray;
}

function formatHash (hash) {
  return isChangeHash(hash) ? hash : '[no change]';
}

function cleanString (text) {
  if (!text) return '';
  return text.trim().replace(/[\n\s]+/g, ' ');
}

function compressPath (targetPath) {
  const cwd = path.dirname(targetPath);
  const inFile = path.basename(targetPath);
  const outFile = `${inFile}.tar.gz`;

  return run('tar', ['-czf', outFile, inFile], {cwd})
    .then(process => {
      if (process.code !== 0) {
        throw new Error(`Failed to compress ${targetPath}: ${process.allIo}`);
      }
      return path.join(cwd, outFile);
    });
}

function sendResults (result) {
  return new Promise((resolve, reject) => {
    const friendlyDate = timeString.replace('T', ' ').replace('Z', ' (GMT)');
    const friendlyTime = friendlyDate.slice(11);
    let friendlyGroup;
    if (tagGroups.length > 2) {
      const last = tagGroups.pop();
      friendlyGroup = tagGroups.join(', ') + `, and ${last}`;
    }
    else {
      friendlyGroup = tagGroups.join(' and ');
    }

    const message = {
      from: `"${senderEmailName}" <${args['--sender-email']}>`,
      to: args['--receiver-email']
    };
    const sourceName = sourceType || 'all';
    const subjectDetails = `${sourceName} versions @ ${friendlyDate} (grouped by ${friendlyGroup})`
    let signature = randomItem([
      '- Your friendly scraperbot',
      '- Your friendly scraperbot',
      '- Scrape-y McScrapesalot',
      '- Your faithful Scraperbot'
    ]);
    signature = `\n\n${signature}\n\n`;

    if (result instanceof Error) {
      message.subject = `[Task Sheets] Error scraping ${subjectDetails}`;

      const greeting = randomItem([
        'Uhoh,',
        'Troubles:',
        'Problems :(',
        'ðŸ’©!'
      ]);

      message.text = `${greeting}\n\nThere was an error scraping the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}:\n\n${result.rawText || result.message}\n\n${result.stack}\n${signature}`;
    }
    else {
      message.subject = `[Task Sheets] Scraped ${subjectDetails}`;
      if (linkToVersionista) {
        message.subject += ' [with Versionista links]'
      }

      const greeting = randomItem([
        'Hi!',
        'Howdy!',
        'Good Morning!',
        'Morninâ€™!',
        'Hey,',
        'Hot off the server!',
        'Headed your way!'
      ]);

      const linkDestination = linkToVersionista ? 'Versionista' : 'the Web Monitoring UI';
      message.text = `${greeting}\n\nI scraped the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}.\nThe links in this data point to ${linkDestination}.\nThe CSVs are grouped by the ${friendlyGroup} tags.\n\n${result.text}${signature}`;
      message.attachments = [{path: result.path}];
    }

    const transporter = nodemailer.createTransport({
      service: 'gmail',
      auth: {
        user: args['--sender-email'],
        pass: args['--sender-password']
      }
    });

    transporter.sendMail(message, (error, result) => {
      if (error) reject(error);
      else resolve(result);
    });
  });
}

function run (command, args, options = {}) {
  return new Promise((resolve, reject) => {
    let allIo = '';
    let stdout = '';
    let stderr = '';

    const child = spawn(command, args, Object.assign(options, {
      stdio: 'pipe'
    }))
      .on('error', reject)
      .on('close', code => {
        resolve({
          code,
          allIo,
          stdout,
          stderr
        });
      });

    child.stdout.on('data', data => {
      allIo += data;
      stdout += data;
      process.stdout.write(data);
    });

    child.stderr.on('data', data => {
      allIo += data;
      stderr += data;
      process.stderr.write(data);
    });
  });
}

function randomItem (items) {
  return items[Math.floor(Math.random() * items.length)]
}

const chunk_expression = /chunk=(\d+)/;

function getChunk (url) {
  return (url.match(chunk_expression) || [])[1] || 1;
}

function getCredentialsFromUrl (url) {
  const matchedCredentials = url.match(/^https?:\/\/([^/@]+):([^/@]+)@/);

  if (matchedCredentials) {
    return {
      username: decodeURIComponent(matchedCredentials[1]),
      password: decodeURIComponent(matchedCredentials[2])
    };
  }

  return null;
}

function alphabeticalSort(a, b) {
  return a.localeCompare(b);
}

function getResponse (apiPath, query) {
  const requestOptions = {};
  if (dbCredentials) {
    requestOptions.auth = dbCredentials;
  }

  return new Promise((resolve, reject) => {
    let url = /^http(s?):\/\//.test(apiPath) ? apiPath : `${dbUrl}${apiPath}`;
    // Serialize the querstring ourselves so we can use it in the cache key.
    if (query) {
      url += '?' + qs.stringify(query, {
        arrayFormat: 'brackets',
        // Sort so we have consistent cache keys.
        sort: alphabeticalSort
      });
    }

    withCache(requestCache, url, getWithRetries, [url, requestOptions, 3], function (error, response) {
      if (args['--debug']) {
        console.log(`Got ${url}`);
      }

      if (error) return reject(error);

      let body;
      try {
        body = JSON.parse(response.body);
      }
      catch (error) {
        throw new Error(
          `Could not parse response for ${url}\n\n${response.body}`);
      }

      if (response.statusCode !== 200) {
        return reject(body.errors[0]);
      };

      resolve(body);
    });
  });
}

// TODO: clean this up?
// TODO: rewrite as promises
function getWithRetries(url, options, retries, callback) {
  if (typeof retries === 'function') {
    [callback, retries] = [retries, 2];
  }
  let retryCount = 0;

  function handleResult (error, response) {
    if ((error || response.statusCode >= 500) && retryCount <= retries) {
      const delay = retryCount > 0 ? 1000 * Math.pow(2, retryCount - 1) : 0
      setTimeout(() => {
        retryCount++;
        request.get(url, options, handleResult);
      }, delay);
    }
    else {
      callback(error, response);
    }
  }

  request.get(url, options, handleResult);
}

// function getAsPromise (url, options) {
//   return new Promise((resolve, reject) => {
//     request.get(url, options, (error, response) => {
//       if (error) return reject(error);
//       resolve(response);
//     });
//   });
// }

// async function withFileCache (basePath, name, operation, args) {
//   const fileName = name.replace(/[\/\\:?()[\]&]/g, '-');
//   const filePath = path.join(basePath, fileName);
//   try {
//     await fs.stat(filePath);
//     const body = await fs.readFile(filePath, {encoding: 'utf8'});
//     return {body, status: 200, statusCode: 200};
//   }
//   catch (error) {
//     const result = await operation(args);
//     await fs.ensureDir(basePath);
//     await fs.writeFile(filePath, result.body, {encoding: 'utf8'});
//     return result;
//   }
// }

function promised (func, ...args) {
  return new Promise((resolve, reject) => {
    func(...args, (error, result) => {
      if (error) return reject(error);
      resolve(result);
    });
  });
}

// TODO: should also rewrite all this cache stuff as a class
// TODO: this should really take a serializer and deserializer
let cacheData = null;
function getCacheObject (cachePath) {
  if (cacheData) return Promise.resolve(cacheData);

  return fs.readFile(cachePath, {encoding: 'utf8'})
    .then(JSON.parse)
    .catch(() => ({}))
    .then(fileData => {
      cacheData = fileData;
      return fileData;
    });
}

let cacheWriteTimer = -1;
const persistCacheAfter = 5000;
async function writeCacheObject (cachePath, cache, now = false) {
  if (now) {
    try {
      // Clear the timer *first* because fs.writeFile is async -- there is an
      // opportunity for new data to be added to the memory cache between when
      // the write finishes and the next instruction. Better to schedule an
      // extra write than to miss a write!
      cacheWriteTimer = -1;
      await fs.ensureDir(path.resolve(cachePath, '..'));
      await fs.writeFile(cachePath, JSON.stringify(cache), {encoding: 'utf8'});
    }
    catch (error) {
      console.error('Failed to write cache:', error);
    }
  }
  else if (cacheWriteTimer === -1) {
    cacheWriteTimer = setTimeout(
      () => writeCacheObject(cachePath, cache, true),
      persistCacheAfter
    );
  }
}

async function getCache (cachePath, key) {
  const cache = await getCacheObject(cachePath);
  const value = cache[key];
  if (value == null) throw new Error(`Unset cache key: ${key}`);
  return value;
}

async function setCache (cachePath, key, value) {
  const cache = await getCacheObject(cachePath)
  cache[key] = value;
  writeCacheObject(cachePath, cache);
}

function withCache (cachePath, name, operation, args, callback) {
  getCache(cachePath, name)
    .catch(async () => {
      const result = await promised(operation, ...args);
      await setCache(cachePath, name, result);
      return result;
    })
    .then(
      result => callback(null, result),
      error => callback(error)
    );
}

async function clearCache (cachePath) {
  try {
    clearTimeout(cacheWriteTimer);
    await fs.remove(cachePath);
  }
  catch (error) {
    // Failure is OK, but log it.
    console.error('Error clearing cache:', error);
  }
}

function delayPromise (timeout) {
  return new Promise(done => timeout ? setTimeout(done, timeout) : done());
}

// Get all pages of a result set from the given API endpoint and query data
function getAllResults (apiPath, qs) {
  return getResponse(apiPath, qs)
    .then(body => {
      if (args['--debug']) {
        console.log(`Got ${getChunk(apiPath)}`);
      }

      // Concatenate the next page onto the end of this one if there is one.
      if (body.links.next) {
        return delayPromise(chunkDelay || 0)
          .then(() => getAllResults(body.links.next))
          .then(nextPages => body.data.concat(nextPages));
      }

      return body.data;
    });
}
