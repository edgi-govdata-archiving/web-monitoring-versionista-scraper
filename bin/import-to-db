#!/usr/bin/env node
'use strict';

const sentryErrors = require('../lib/sentry-errors').setup();

const fs = require('fs');
const path = require('path');
const stream = require('stream');
const url = require('url');
const neodoc = require('neodoc');
const parallel = require('parallel-transform');
const pump = require('pump');
const request = require('request');
const split = require('split');

const args = neodoc.run(`
Sends the contents of a JSON-stream versions file generated by
scrape-versionista to an instance of web-monitoring-db.

Usage: import-to-db [options] <paths>...

Options:
  -h, --help       Print this lovely help message.
  --email EMAIL    E-Mail for web-monitoring-db [env: WEB_MONITORING_EMAIL]
  --password PASS  PASSWORD for web-monitoring-db [env: WEB_MONITORING_PASSWORD]
  --host HOST      Alternate host name for web-monitoring-db. Use this to send
                   data to an alternate instance of the DB.
                   [default: https://api.monitoring.envirodatagov.org/]
                   [env: WEB_MONITORING_URL]
  --bucket BUCKET  Amazon S3 bucket that is hosting raw version data.
                   [default: edgi-wm-versionista]
                   [env: AWS_S3_BUCKET]
  --chunk SIZE     Send versions in chunks of this size. [default: 1000]
  --update UPDATE  Set the update behavior, which determines how versions that
                   are already in the database are treated. Should be one of
                   'skip' (default), 'merge', or 'replace'.
`);

const dbUrl = composeUrl(
  args['--host'],
  args['--email'],
  args['--password'],
  'api/v0/imports');
const chunkSize = args['--chunk'];
const versionFilePaths = args['<paths>'];
const bucket = args['--bucket'];
const startDate = Date.now();

function importableVersion (version) {
  let s3Url = undefined;
  if (version.filePath) {
    let s3Path = [
      version.account,
      `${version.siteId}-${version.pageId}`,
      path.basename(version.filePath)
    ].join('/');
    s3Url = `https://${bucket}.s3.amazonaws.com/${s3Path}`;
  }

  return {
    page_url: version.pageUrl,
    page_maintainers: [version.agency],
    page_tags: [`site:${version.siteName}`],
    title: version.title || version.pageTitle,
    capture_time: version.date,
    uri: s3Url,
    version_hash: version.hash,
    status: version.status,
    source_type: 'versionista',
    source_metadata: {
      account: version.account,
      site_id: version.siteId,
      page_id: version.pageId,
      version_id: version.versionId,
      url: version.url,
      has_content: version.hasContent,
      error_code: version.is404Page ? '404' : version.errorCode,
      diff_with_previous_url: version.diffWithPreviousUrl,
      diff_with_first_url: version.diffWithFirstUrl,
      diff_hash: version.diff && version.diff.hash,
      diff_length: version.diff && version.diff.length,
      diff_text_hash: version.textDiff && version.textDiff.hash,
      diff_text_length: version.textDiff && version.textDiff.length,
      length: version.length,
      headers: version.headers,
      content_type: version.contentType,
      status: version.status,
      load_time: version.loadTime,
      redirects: version.redirects,
      last_date: version.lastDate
    }
  };
}

let exitCode = 0;
let versionsImported = 0;
let versionsTotal = 0;
let filesTotal = 0;

function importJsonFile (filePath, callback) {
  pump(
    fs.createReadStream(filePath),
    split(line => (line === '' ? null : JSON.parse(line))),
    mapStream(importableVersion),
    chunkedObjectStream(chunkSize),
    parallel(5, importIntoDb),
    mapStream(importResult => {
      const errors = importResult.processing_errors;
      versionsTotal += importResult.processed_versions;
      versionsImported += importResult.processed_versions - errors.length;

      if (errors.length) {
        exitCode = 1;
        errors.forEach(error => console.error(error));
        // sentryErrors.captureMessage(`${errors.length} import errors: ${errors.join(', ')}`);
        sentryErrors.captureMessage(
          `${errors.length} import errors`,
          {extra: {errors: errors}}
        );
      }

      console.log(`Completed ${versionsTotal} versions.`);
    }),
    // Unfortunately, pump only waits for the last stream's write end to finish,
    // not for it to close or for its read end to end. This passthrough stream
    // ensures the stream above finishes all its work before the callback :/
    stream.PassThrough({objectMode: true}),
    callback
  );
}

function complete (error) {
  if (error) {
    exitCode = 1;
    console.error(error);
  }

  console.error(`Completed in ${(Date.now() - startDate) / 1000} seconds.`);
  console.error(`  ${versionsImported} of ${versionsTotal} imported from ${filesTotal} files.`);
  sentryErrors.flush().then(() => process.exit(exitCode));
}

function processNextFile () {
  const filePath = versionFilePaths.shift();
  if (filePath) {
    console.error(`Loading "${filePath}"`);
    importJsonFile(filePath, error => {
      filesTotal++;
      if (error) {
        return complete(error);
      }
      processNextFile();
    });
  }
  else {
    complete();
  }
}

// GO!!!!
processNextFile();


// HELPERS -----------------

function mapStream (mapper) {
  return stream.Transform({
    objectMode: true,
    transform (data, encoding, callback) {
      try {
        const result = mapper(data);
        if (result != null) {
          this.push(result);
        }
        callback();
      }
      catch (error) {
        return callback(error);
      }
    }
  });
}

function filterStream (predicate) {
  return stream.Transform({
    objectMode: true,
    transform (data, encoding, callback) {
      try {
        if (predicate(data)) {
          this.push(data);
        }
        callback();
      }
      catch (error) {
        return callback(error);
      }
    }
  });
}

function chunkedObjectStream (chunkSize = Infinity) {
  if (chunkSize < 1 || typeof chunkSize !== 'number') {
    chunkSize = 1;
  }

  return stream.Transform({
    objectMode: true,
    transform (data, encoding, callback) {
      if (!this._chunkBuffer) {
        this._chunkBuffer = [];
      }
      this._chunkBuffer.push(data);
      if (this._chunkBuffer.length === chunkSize) {
        this.push(this._chunkBuffer);
        this._chunkBuffer = [];
      }
      callback();
    },
    flush (callback) {
      if (this._chunkBuffer && this._chunkBuffer.length) {
        this.push(this._chunkBuffer);
      }
      this._chunkBuffer = null;
      callback();
    }
  });
}

function composeUrl (sourceUrl, user, password, tail = '') {
  const parsedUrl = url.parse(sourceUrl);

  if (!parsedUrl.auth) {
    parsedUrl.auth = `${user}:${password}`;
  }

  if (!parsedUrl.pathname.endsWith('/')) {
    parsedUrl.pathname += '/';
  }
  parsedUrl.pathname += tail;

  return url.format(parsedUrl);
}

function importIntoDb (versions, callback) {
  const qs = args['--update'] ? {update: args['--update']} : {};

  request({
    method: 'POST',
    url: dbUrl,
    headers: {'Content-Type': 'application/x-json-stream'},
    body: versions.map(v => JSON.stringify(v)).join('\n'),
    qs
  }, (error, response, rawBody) => {
    if (error) {
      return callback(error)
    }

    let body;
    try {
      body = JSON.parse(rawBody);
    }
    catch (error) {
      error.message = `${error.message} (while parsing: \`${rawBody}\`)`;
      throw error;
    }

    if (body.errors) {
      return callback(body.errors[0]);
    }

    const importId = body.data.id;
    const poll = (callback) => {
      setTimeout(() => {
        request({
          url: `${dbUrl}/${importId}`,
          json: true
        }, (error, response, body) => {
          if (error) {
            return callback(error);
          }
          else if (body.data.status !== 'complete') {
            return poll(callback);
          }

          body.data.processed_versions = versions.length;
          callback(null, body.data);
        });
      }, 1000);
    };

    poll(callback);
  });
}
